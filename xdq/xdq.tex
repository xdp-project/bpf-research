%%
%% This is file `sample-xelatex.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-xelatex.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%

\documentclass[sigconf, nonacm]{acmart}

\usepackage{mwe}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\usepackage{cleveref}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Express Data Queueing}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Freysteinn Alfreðsson}
\email{freysteinn.alfredsson@kau.se}
\orcid{0000-0003-1516-9370}
\affiliation{%
  \institution{Department of Computer Science}
  \city{Karlstad}
  \country{Sweden}
}
\author{Per Hurtig}
\email{per.hurtig@kau.se}
\affiliation{%
  \institution{Department of Computer Science}
  \city{Karlstad}
  \country{Sweden}
}
%%\orcid{0000-0003-1516-9370}
\author{Anna Brunstrom}
\email{anna.brunstrom@kau.se}
\affiliation{%
  \institution{Department of Computer Science}
  \city{Karlstad}
  \country{Sweden}
}

\author{Toke Høiland-Jørgensen}
\email{toke@redhat.com}
\affiliation{%
  \institution{Red hat}
  \country{Denmark}
}

\author{Jesper Dangaard Brouer}
\email{brouer@redhat.com}
\affiliation{%
  \institution{Red hat}
  \country{Denmark}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Alfreðsson, et al.}

\begin{abstract}
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{XDP, BPF, BPF, Queueing, Linux, Scheduling}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

The Linux kernel is a widely used platform in devices that range from IoT equipment, cellphones, home and enterprise routers to servers and cloud offerings. One of the prominent technologies of the Linux kernel is the BPF framework~\cite{ebpf}, which gives us a flexible way to extend the kernel. This framework is an in-kernel runtime environment that allows domain-specific code to execute within the kernel safely using predefined hooks. One such hook is the Linux eXpress Data Path~\cite{hoiland2018express}, or XDP, which introduces a fast data-path within the Linux kernel and has found numerous uses in the industry such as Denial-of-Service (DoS) attack mitigation, load-balancers, and intrusion prevention systems.

XDP provides a high-performance programmable network data path and allows programmers to process packets early out of the driver and even bypass the kernel's network stack for increased performance. While XDP excels in forwarding packets, it currently has no mechanism for queuing or reordering packets and can not implement traffic scheduling policies. Packet scheduling is a common task on network equipment, and most devices that provide packet scheduling algorithms provide only a handful of predefined packet schedulers. Therefore, there is a growing interest in bringing more flexible programmability to this domain. The kernel's network stack provides a traffic control system called Queuing discipline (Qdisc) capable of packet scheduling. However, it does not provide the means of implementing complete packet schedulers in BPF, nor can it be used in XDP when bypassing the kernel's networking stack.

% Frey: This sentence stops a bit too abruptly.

This paper presents our work on adding programmable packet scheduling to XDP. We have designed a programmable packet scheduling framework in BPF using recently proposed schemes for programmable queues. This extension allows programmers to define their packet schedulers using BPF while benefiting from the XDP fast data path.

% Frey: The last paragraph could be rewritten to list the contributions.
% - A BPF packet scheduling framework.
% - XDP with queueing using the new framework.
% - Qdisc with queueing using the new framework.
% - Evaluation of the new framework.

\todo[inline]{Frey: A reminder that this section should end with an overview of the structure of the paper when it has been finalized.}

\section{Packet Scheduling, Traffic Shaping, and Queue Management}

Packet schedulers provide the means of controlling the policy and quality requirements within a network for customers and services. However, at its core, the primary function of the packet scheduler is to arrange or rearrange packets to meet the requirements of the network. Most of these requirements are to improve the latency of specific flows or to improve fairness between flows. For instance, video streaming services would like to ensure that clients get fair access to their video streams. Moreover, a service provider might want to prioritize traffic of their productions servers over their testing servers. Therefore, having the flexibility to adapt the packet scheduling algorithm to the requirements of the network in question can significantly improve its overall quality.

\begin{figure}
  \includegraphics[width=\linewidth]{round-robin.pdf}
  \caption{The Round-Robin scheduler sorts enqueued packets into different queues depending on their flows. On dequeue, the packets are dequeued in a round-robin fashion, giving equal throughput for flows when all packets are equal in size.}
  \label{fig:round_robin}
\end{figure}

For example, if we wanted to give equal throughput to three different clients that have equal size packets, we could use the round-robin scheduler\cite{nagle1987packet}, depicted in \cref{fig:round_robin}. This scheduler sorts the incoming packets by flow into different queues, and dequeues the packets by taking one packet from each flow at the time.

%\begin{figure}
%  \includegraphics[width=\linewidth]{traffic-policy.pdf}
%  \caption{\label{fig:traffic_policy}}
%\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{traffic-shaping.pdf}
  \caption{Traffic shaping is the art of delaying packets to control the throughput. The figure depicts traffic without shaping on the left and the same traffic with shaping on the right.}
  \label{fig:traffic_shaping}
\end{figure}

Another crucial component related to packet scheduling is the capability of delaying packets. This capability allows packet scheduling algorithms to provide traffic shaping, as seen in \cref{fig:traffic_shaping}. Shaping algorithms are referred to as non-work conserving algorithms because they do not necessarily use the total capacity of the network interface, while work-conserving algorithms never delay packets and can use their total capacity. An example of a non-work conserving scheduling algorithm would be an ISP enforcing a network policy that guarantees their clients their bandwidth.

Queue management is also an essential topic related to packet scheduling and is the art of controlling how many packets queues can queue. A significant problem in today's environments is bufferbloat, where network vendors have previously favored large buffers for throughput by sacrificing latency. Bufferbloat can negatively affect low latency applications such as video conferencing, online gaming, and other interactive applications. Therefore, queue management is an integral part of the queueing process, while it is not usually considered part of the packet scheduling algorithm.

% Frey: This last paragraph should contain a bit more about what bufferbloat is.

\section{The Linux Kernel Networking Subsystem}

\begin{figure}
  \includegraphics[width=0.6\linewidth]{network-overview.pdf}
  \caption{XDP allows network application programmers to write code that runs on every network packet using the BPF framework. XDP gives the programmer the capability to modify the packet and redirect the packet using the code's return value. The possible return values are as follows: 1) Passing the packet to the kernel's networking stack. 2) Redirecting the packet to another network device. 3) Send the packet directly out on the same device. 4) Or dropping the packet altogether.}
  \label{fig:network_overview}
\end{figure}

The Linux kernel networking stack is highly mature and a flexible piece of software. It contains a hardware abstraction layer, a traffic control layer responsible for routing and firewall rules, and layers responsible for handling the TCP/IP networking layers. Part of the traffic control layer is the queueing discipline layer or the Qdisc layer for short. This layer gives the Linux kernel flexible packet scheduling capabilities and an interface to create new packet schedulers as loadable kernel modules. However, due to the immense speed increases in modern network devices, the kernel's networking stack has become a bottleneck. This obstacle has prompted network vendors and researchers to create alternative solutions to mitigate this performance bottleneck. One such solution is the Data Plane Development Kit~\cite{dpdk} (DPDK) which bypasses the Linux kernel and communicates directly to the networking hardware. An alternative solution to bypassing the Linux kernel, which we describe in \cref{sec:xdp}, is XDP, which creates a fast data path within the Linux kernel while retaining the kernel and user-space separation. We provide a simplified diagram of the Linux kernel's networking infrastructure in \cref{fig:network_overview} which shows the relevant parts related to our work in this paper. It shows how XDP can steer traffic either through the network stack or bypass it altogether.

% Frey: The idea of this paragraph was to prepare a bit of differences between the BPF queuing implementation between the Qdisc and XDP. However, I am commenting this section out until we have more implementation information and see how relevant this section is at that point.
% The primary networking data structure in the Linux kernel is the sk\_buff. The sk\_buff is a large data structure with multiple fields related to all layers in the network stack and gets populated through each layer of the network stack, including the Qdisc layer. However, XDP handles network packets in the driver right after a DMA synchronization. Therefore, it does not use the sk\_buff data structure but represents the packet using a slimmer data structure called xdp\_frame, which does not get converted into a sk\_buff unless the XDP hook passes the packet onto the network stack.


\subsection{The Linux Kernel Packet Queueing Discipline (Qdisc)}

% Frey: Add a Figure that shows the relationship of classfull and classless hierarchies and how a classifier queues traffic into those classless Qdiscs.
\todo[inline]{Frey: I haven't drawn the Qdisc figure in this document.}

The Linux kernel networking stack handles traffic control using the Qdisc subsystem. It is a mature system with multiple capabilities, such as policing, shaping, classifying, dropping, and marking traffic. While the Qdisc subsystem has more capabilities than packet scheduling, we will only introduce the three main components related to scheduling. These components are classfull Qdiscs, classless Qdiscs, and classifiers. These three components allow us to define complicated packet schedulers but with limited programmable capabilities. As seen in \cref{fig:qdisc_overview}, classfull Qdiscs are used to create hierarchies of Qdiscs where the leaf nodes are classless Qdiscs. Classifiers are attached to classfull Qdiscs to determine where to queue within the packet within the hierarchy.

% Frey: Here I need a bit more text explaining classfull and classless Qdiscs.

%\section{Packet Schedulers in Practice}
%
%\todo[inline]{Frey: This section needs a new name. The idea with this section is to explain why we care about this work and what the requirements are.}
%
%\begin{itemize}
%  \item Explain what is not possible using today's packet schedulers.
%  \item Explain why you would like to be able to schedule traffic
%\end{itemize}


\section{Programmable Packet Schedulers}

\begin{figure}
  \includegraphics[width=0.5\linewidth]{pifo-hierarchy.pdf}
  \caption{We can implement complex packet scheduling algorithms by forming hierarchies of PIFO and the Eiffel data structures. Some packet schedulers need hierarchies to function. However, we can also use hierarchies to combine different schedulers to handle complex requirements.}
  \label{fig:pifo_hierarchy}
\end{figure}

Traditionally, network equipment and operating systems provide a handful of packet scheduling algorithms. This limited number of schedulers forced companies to tune the parameters of these algorithms to meet their requirements. However, modern networking equipment has started to provide programmable network capabilities, which the network engineer can leverage using specialized programming languages such as P4~\cite{p4} or standard programming languages such as C, such as in the case of traditional operating systems. This new offering allows the network designers to create custom logic and custom packet scheduling solutions optimized for their specific unique use case. Therefore, instead of tuning the network to the limitations of the packet scheduling algorithm, they can tune their packet scheduling algorithm directly to their network requirements.

Consequently, the building blocks of a programmable packet scheduling framework need to be straightforward, fast, and easy to understand. Therefore, the essential part of creating a programmable packet scheduling framework is to provide a flexible data structure. Therefore, this has become an active research area to develop a flexible data structure that programmers can leverage to implement their packet schedulers. However, due to the vast speeds of modern network interfaces, severe time limitations are put on these data structures, which tend to make generic priority queues, such as red-black trees and binary heaps, unsuitable choices.

In this paper, we focus on the two priority queues, PIFO and Eiffel, described in \cref{sec:pifo} and \cref{sec:eiffel}. Both are simple data structures that share the following qualities: (i) They use an integer-based ranking function to queue packets by their priority and rely on a fixed range of ranks known at initialization time; and (ii) they dequeue according to the scheduled packet order. Furthermore, they both have the flexibility to create complex packet scheduling algorithms by using hierarchies, as seen in \cref{fig:pifo_hierarchy}. However, Eiffel differs in that it can make scheduling decisions on dequeue, while PIFO only makes scheduling decisions on enqueue. This distinction stems from the fact that PIFOs are well suited for both hardware and software. However, Eiffel is a software-only data structure and is, therefore, capable of more complex operations, such as flow-based scheduling, which requires scheduling on dequeue.


\subsection{PIFO and SP-PIFO} \label{sec:pifo}

Despite its simplicity, the Push-In First-Out data structure\cite{Sivaraman2016} is quite versatile and allows the programmer to implement a wide range of packet schedulers. From a programmer's perspective, the programmer only needs to decide what order to schedule packets and when to schedule them for non-work conserving algorithms. However, one limitation of the PIFO is that each FIFO, and therefore, each rank takes memory. Therefore, the more granularity of ranks, the more resources the PIFO needs. This limitation limits how large the PIFO can be both in hardware and software implementations. A mitigation to this limitation is the SP-PIFO\cite{Alcoz2020}, which approximates a large PIFO using a smaller PIFO.

\begin{figure}
  \includegraphics[width=0.25\linewidth]{pifo.pdf}
  \caption{\todo[inline]{I think we should discuss better ways to draw both the PIFO and Eiffel data structures. I think it might be better to have two different pictures—one for PIFO and another for Eiffel with references to queues.}}
  \label{fig:pifo}
\end{figure}

\subsubsection{Data Structure}

The PIFO is a priority queue where the programmer can queue packets based on their rank, where each rank is queued in a FIFO order, as shown in \cref{fig:pifo}. However, the programmer can only dequeue from the head of the PIFO and only order packets on enqueue. However, the PIFO does not allow reordering the packets within a FIFO. Nor can it reorder the packets on deqeue. This limitation does exclude the PIFO from implementing some packet scheduling algorithms, such as pFabric\cite{alizadeh2013pfabric}.


\subsection{Eiffel} \label{sec:eiffel}

The Eiffel data structure is an efficient software implementation of a PIFO with a couple of alterations. First, in Eiffel, the programmer can schedule flows or packets, depending on the algorithm, where each flow is a separate FIFO that contains only the flow's packets. The Eiffel data structure can schedule references to those FIFOs instead of the individual packets. Second, the programmer can do on-dequeue scheduling; this allows the algorithm to virtually rearrange all the packets from the same flow by simply requeueing the reference to that flow back into the Eiffel data structure. These two additions allow the Eiffel data structure to represent more scheduling algorithms than PIFO, such as pFabric\cite{alizadeh2013pfabric}.

Also, because Eiffel is a software implementation, the Eiffel paper proposes a few software-based optimizations to improve the performance of the Eiffel data structure. One such optimization is to use a bit-field to keep track of queues containing packets and use the CPUs Find-First-Set instruction to accelerate the lookup in the bit lookup table. The lookup table can be further segmented into a tree-based lookup table when the Eiffel has many ranks.


\section{The BPF framework}

One of the fundamental problems with extending the Linux kernel with specialized code, such as packet schedulers, is writing the extensions as kernel modules. From a security perspective, the modules have access to the entire kernel without limits. Therefore, it is more prone to programming errors, such as pointer mishandling or ending up in infinite loops. The kernel community has created a safer way of extending the kernel called BPF. This framework is an in-kernel runtime environment that allows specialized programs to execute in the kernel safely. To support BPF, kernel programmers add appropriate BPF hooks to the kernel to offer domain-specific extendability to programmers. These hooks allow the programmers to attach BPF code that runs each time these hooks are triggered. Currently, the Linux kernel comes with numerous hooks related to networking, tracing, and security. Where each hook limits what the BPF code can access and which helper functions it can call.

From a developer perspective, BPF programs are binaries written in the BPF instruction set that can be loaded using the bpf system call into the kernel. This instruction set and runtime are severely limited in functionality. It supports a handful of instructions, and the runtime only has a 512-byte stack. When the kernel loads the binary, it runs it through a BPF verifier that makes sure that all pointers are within bounds, that the program does not call disallowed functions, and that the code cannot run more than a million instructions. After verifying the code, the user-space application can attach the BPF program to the desired hook. However, because each hook provides a different method for attaching BPF programs, it is recommended to use the libbpf library to load the programs instead.

A salient feature of BPF is that it offers interprocess communication using BPF maps. These maps are key-value stores that act as global variables within the framework and are the primary method to interact with BPF programs. They come in different types, such as arrays and hashmaps, and some of the more advanced maps offer per-CPU storage for additional performance. They provide a common way for different BPF programs to communicate and user-space applications to communicate with the BPF programs.


\subsection{Express Data Path (XDP)} \label{sec:xdp}

XDP\cite{hoiland2018express} is a type of BPF program that is attached directly to the RX path of a network interface to allow the programmer direct manipulation of packets from the network driver. Its primary use cases are high-performance packet processing and the ability to bypass the kernel's network stack by redirecting the packet to different locations, such as back out the same device, to another device, or dropping the packet entirely. XDP also provides helper functions that allow the programmer to call particular parts of the network stack, such as routing lookups.

While XDP excels at forwarding packets, it does not support packet reordering or packet scheduling. Our objective in this paper is to extend XDP to support packet scheduling. This support includes adding helper functions and extra hooks for dequeuing and pacing operations.


\subsection{BPF Qdisc Classifiers}

Currently, the Qdisc layer only has a BPF classifier capable of manipulating packets and directing them into excising Qdiscs that are not programmable using BPF. While this does provide a form of programable packet scheduling, it relies exclusively on the Qdisc layer, which XDP can not use. We aim to provide a BPF based packet scheduling framework for XDP and as a standalone Qdisc.


\section{A Programmable Packet Scheduling Framework in BPF}

\begin{figure}

  \includegraphics[width=\linewidth]{bpf_pps_flow.pdf}

  \caption{Depicted are the BPF hooks needed to implement programmable packet scheduling using our framework. The main hooks are the enqueue and dequeue hooks, with the optional timer hook. Queueing works the same in both XDP and the Qdisc. However, they do not use the same enqueue and dequeue hooks.}
  \label{fig:bpf_pps_flow}
\end{figure}

We have created an BPF based programmable packet scheduling framework that is flexible enough to share between the XDP and kernel Qdisc layers. The design is depicted in \cref{fig:bpf_pps_flow}, which shows the basic building blocks for our design without the specifics of XDP nor the Qdisc subsystem.

These building blocks are then further broken down as follows:

\begin{itemize}
        \item Queue map: This new BPF map implements a PIFO and is the main building block for the programmer to create packet schedulers.
        \item Enqueue hook: This BPF hook is responsible for redirecting packets to BPF queue maps. This is the standard XDP hook in the case of XDP. However, it is a new hook in Qdiscs.
        \item Timer hook: This is an optional hook and is only required for algorithms that delay packets, e.g., packet pacing or traffic shaping. It is responsible for dequeuing and enqueuing packets from different PIFOs to introduce delayed packets. We implement timers using the newly introduced BPF timer API. This API allows the programmer to schedule BPF programs to be run at a specified time in the future, which can be used to release held packets.
        \item Dequeue hook: This hook is responsible for delivering the packet from the packet scheduling algorithm. In XDP, this is a new hook that each driver calls to transmit a packet. It can deliver bulking by repeatedly calling the hook to dequeue multiple packets before it transmits them.
\end{itemize}

Our design represents the PIFO data structure as an eBPF map. This representation allows the BPF programs to use a familiar map interface to reference PIFOs straightforwardly in their scheduling algorithm. From a programmer's perspective, the eBPF hooks reference the queues like any other map type. The enqueue hook decides which queue to direct the packet to by a map reference. Similarly, the dequeue hook picks which queue map to dequeue from and returns a reference to the dequeued packet to the kernel for transmission. For shaping scheduling algorithms, the BPF packet scheduling hooks set BPF timers that trigger BPF programs at specified times. The BPF timer API supports nanosecond precision using the kernel's \textit{hrtimers}, making it ideal for our use case.


\section{BPF-Qdisc}

\todo[inline]{Frey: I am leaving this section here for now. My original idea was that this section would detail how we implemented queueing in XDP. This could include what drivers were changed and where in the pipeline we did the changes.}

To support our new queueing API both for forwarded packets and through locally generated traffic, we provide a separate BPF-Qdisc. This Qdisc still relies on the Linux kernel and does not use XDP. While it does not use the same hooks as XDP, it allows the packet scheduling programmer to use the same queueing API.


\section{XDP Queueuing}

\todo[inline]{Frey: I am leaving this section here for now. My original idea was that this section would detail how we implemented queueing in XDP. This could include what drivers were changed and where in the pipeline we did the changes.}

Our design extends XDP by adding a new dequeue hook into the network drivers. We rely on standard XDP to enqueue packets into BPF queue maps. However, the new dequeue hook supports bulking by having the driver repeatedly dequeue packets from the dequeue hook before it transmits the packets.


\section{Evaluation of BPF-Qdisc and XDP Queueing}

\begin{itemize}
  \item Compare a type of scheduling algorithm using BPF-Qdisc and XDP Queueing performance to Qdiscs that does the same functionality.
  \item Compare BPF-Qdisc and XDP Queueing performance.
\end{itemize}

\section{Conclusion}

\begin{itemize}
  \item Reiterate main points of design, and summarize results.
\end{itemize}



\bibliographystyle{ACM-Reference-Format}
\bibliography{references}


\end{document}
\endinput
